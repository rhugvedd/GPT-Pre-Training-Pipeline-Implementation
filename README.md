# GPT Model Development from Scratch  

## ‚ö†Ô∏è Important Notice  

Apologies for the delay! I originally uploaded the [**SansGPT**](https://aclanthology.org/2024.icon-1.50/) project repo in **December 2024**, but I forgot to upload the **generalized version** of the GPT project to this repo at that time.  

I had previously uploaded the **SansGPT-specific** files to the **SansGPT repository** [here](https://github.com/rhugvedd/SansGPT-Advancing-Generative-Pre-Training-in-Sanskrit) in December 2024, but the generalized versions were unintentionally left out.  

This repository now includes all the **missing files**, which have only **minor differences** compared to the SansGPT files.  

These files were **developed before December 2024**‚Äîthey are **not recent additions**. I sincerely apologize for any inconvenience caused.  

---  

## Overview  

This repository contains the complete implementation of a **GPT (Generative Pre-trained Transformer) model**, built from scratch. The project is designed to provide a **fundamental and rigorous understanding** of the core components of a GPT-based architecture.  

It includes key stages such as **tokenization, pre-training, supervised fine-tuning**, covering both theoretical insights and practical implementation.  

## üìÖ Development Timeline  

The project was actively developed **from February 2024 to December 2024**. All the files in this repository were created before December 2024, and this upload ensures their availability.  

## üöÄ Project Goals  

- **Tokenization**  
  Implement and analyze different tokenization techniques (including Byte Pair Encoding) to optimize text preprocessing for training deep learning models.  
- **Pre-Training**  
  Develop the core GPT architecture and implement a **pre-training pipeline** for learning language representations.  
- **Supervised Fine-Tuning**
  Fine-tune the pre-trained model on specific tasks to improve its performance in domain-specific applications.  
- **Reinforcement Learning**  
  Explore reinforcement learning techniques for improving model performance beyond supervised learning.  

## üìÇ Repository Structure  

This repo includes:  

- **BPETokenizer.py** ‚Üí Tokenization module (Byte Pair Encoding)  
- **DataLoader.py** ‚Üí Data loading and preprocessing utilities  
- **FineTuneConfig.py / TrainConfig.py** ‚Üí Configuration files for fine-tuning and training  
- **FineTuner.py / Trainer.py** ‚Üí Implementation of fine-tuning and pre-training logic  
- **Fine-Tune-Pipeline.py / Pre-Train-Pipeline.py** ‚Üí Scripts for executing fine-tuning and pre-training pipelines  
- **Transformer.py** ‚Üí Core GPT model implementation  
- **TrainVocab.py** ‚Üí Vocabulary training module  
- **README.md** ‚Üí Project documentation  

## üì¨ Contact  

For any questions, feedback, or suggestions, feel free to **open an issue** in this repository or contact me at:  

üìß [rhugved.c@neuralcurve.com](mailto:rhugved.c@neuralcurve.com)  